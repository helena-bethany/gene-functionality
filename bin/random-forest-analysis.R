# Script Name: random-forest-analysis.R
#
# Author: Helena Cooper
# Last edited: 27/08/2020
#
# Description: This script runs randomForest 100 times and calculates the performance metrics for these models.
#
# Output: Four files are generated from this script in total.
#         The "stats" file is the minimum, average and maximum performance metrics across all 100 random forest models.
#         The "importance" file is the variable importances recorded for each model, in addition to the average.
#         The "error" file contains the OOB values for each model, in addition to the average.
#         The "predictions" file contains the sum of all confusion matrices generated by the models.
#

###########################################################################################################################

#set.seed(1000)   # Reproducibility while debugging

######## Specify requried packages 
packages <- c("randomForest", "epiR","pROC", "mltools")

######## Now load or install&load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

######## Function for calculating the F1 score from confusion matrix (data)

f1_score <- function(data){
  TP <- data[1,1]
  FP <- data[1,2]
  FN <- data[2,1]
  (2*TP)/((2*TP)+FP+FN)
}

######## Function for calculating accuracy from confusion matrix (data)

accuracy_score <- function(data){
  TP <- data[1,1]
  FP <- data[1,2]
  FN <- data[2,1]
  TN <- data[2,2]
 (TP+TN)/(TP+TN+FP+FN)
}

######## Function for running randomForest for generated feature data (file1)

randomforestx <- function(file1){

  # General setup
  data <- read.csv(file1, header=TRUE)     
  d <- Sys.Date()    # Date variable 
  name=substr(file1,8,1000)  # Name of the dataset analysed (eg: protein-coding-exon2-dataset.csv)
  file2=paste(d,"importance",name,sep='-')    # Name of importance file output
  file3=paste(d,"error",name,sep='-')         # Name of error file output
  file4=paste(d,"predictions",name,sep='-')   # Name of predictions file output
  file5=paste(d,"stats",name,sep='-')         # Name of stats file output
 
  # Create blank vectors for all performance metrics, so average can be calculated
  sens_interim <- c() ; spec_interim <- c() ; acc_interim <- c() ; mcc_interim <- c() ; f1_interim <- c() ; auc_interim <- c()

  # Reorder sequence functionality so that TP is the correct classification of functional sequences (Yes) and TN is for non-functional sequences (No). 
  data$Functional <- factor(data$Functional, labels=c("Yes","No"))
  
  # Run randomforest
  ind <- sample(2,nrow(data),replace=TRUE,prob=c(0.7,0.3))    # Split dataset into test and train dataset, replace=TRUE to prevent duplicates.
  trainData <- data[ind==1,]
  testData <- data[ind==2,]
  data.rf <- randomForest(Functional~.,data=trainData,ntree=1000,proximity=TRUE, na.action=na.roughfix)   # NA values approximated by randomForest.
  
  total <- round(importance(data.rf),4)   # Gini variable importance values
  err <- data.rf$err.rate[1000,]          # OOB errors
  dataPred <- predict(data.rf, newdata=testData)   # Predictions from model
  pred <- table(dataPred,testData$Functionality)   # Confusion table
  
  # MCC calculation
  mcc_input <- cbind(dataPred,testData$Functional)  # Prediction and actual functionality for testData
  mcc_input <- na.omit(mcc_input)    # Can't deal with sequences with approximated NA values
  mcc_value <- mcc(mcc_input[,1],mcc_input[,2])
  min_mcc <- mcc_value ; max_mcc <- mcc_value    # Assign min/max values
  
  # F1 score calculation
  f1 <- f1_score(pred)
  min_f1 <- f1 ; max_f1 <- f1   # Assign min/max values
  
  # Accuracy calculation
  acc <- accuracy_score(pred)
  min_acc <- acc ; max_acc <- acc   # Assign min/max values
  
  # AUC calculation
  resultsProb <- as.data.frame(predict(data.rf, testData, type="prob"))  # Classification/Prediction probabilities required for AUC calculation
  rocYes <- roc(testData$Functional,resultsProb$Yes,smoother=TRUE) 
  auc = rocYes$auc
  min_auc <- auc ; max_auc <- auc   # Assign min/max values
  
  # Sensitivity and Specificity calculations
  rev <- epi.tests(pred,conf.level=0.95)
  sum_rev <- summary(rev)    # Use summary to get more decimal places
  sens <- sum_rev[3,1] 
  spec <- sum_rev[4,1]
  max_sens = sens ; min_sens = sens  # Assign min/max values
  max_spec = spec ; min_spec = spec  # Assign min/max values
  
  # Append performance metrics to appropriate vectors
  sens_interim <- append(sens_interim, sens) ; spec_interim <- append(spec_interim, spec) ; auc_interim <- append(auc_interim, auc)
  mcc_interim <- append(mcc_interim, mcc_value) ; acc_interim <- append(acc_interim, acc) ; f1_interim <- append(f1_interim, f1)

  n <- 1    # Count for number of randomForest models generated

  while (n < 100) {
    
    # Run randomforest
    ind <- sample(2,nrow(data),replace=TRUE,prob=c(0.7,0.3))    # Split dataset into test and train dataset, replace=TRUE to prevent duplicates.
    trainData <- data[ind==1,]
    testData <- data[ind==2,]
    data.rf <- randomForest(Functional~.,data=trainData,ntree=1000,proximity=TRUE, na.action=na.roughfix)   # NA values approximated by randomForest.
  
    value <- round(importance(data.rf),4)   # Gini variable importance values
    total <- cbind(total,value)   # Append new importance values to those from previous models
    err2 <- data.rf$err.rate[1000,]          # OOB errors
    err <- rbind(err,err2)   # Append new OOB errors to those from previous models
    dataPred <- predict(data.rf, newdata=testData)   # Predictions from model
    pred2 <- table(dataPred,testData$Functionality)   # Confusion table
    pred <- pred + pred2  # Sum new confusion tables with those from previous models
    
    # MCC calculation
    mcc_input <- cbind(dataPred,testData$Functional)  # Prediction and actual functionality for testData
    mcc_input <- na.omit(mcc_input)    # Can't deal with sequences with approximated NA values
    mcc_value <- mcc(mcc_input[,1],mcc_input[,2])
    min_mcc <- min(mcc_value, min_mcc) ; max_mcc <- max(mcc_value, max_mcc)  # Only assign min/max if smaller/greater than current assigned variable

    # F1 score calculation
    f1 <- f1_score(pred)
    min_f1 <- min(f1, min_f1) ; max_f1 <- max(f1, max_f1) # Only assign min/max if smaller/greater than current assigned variable
  
    # Accuracy calculation
    acc <- accuracy_score(pred)
    min_acc <- min(acc, min_acc) ; max_acc <- max(acc, max_acc) # Only assign min/max if smaller/greater than current assigned variable
  
    # AUC calculation
    resultsProb <- as.data.frame(predict(data.rf, testData, type="prob"))  # Classification/Prediction probabilities required for AUC calculation
    rocYes <- roc(testData$Functional,resultsProb$Yes,smoother=TRUE) 
    auc = rocYes$auc
    min_auc <- min(auc,min_auc) ; max_auc <- max(auc,max_auc)  # Only assign min/max if smaller/greater than current assigned variable

    # Sensitivity and Specificity calculations
    rev <- epi.tests(pred,conf.level=0.95)
    sum_rev <- summary(rev)    # Use summary to get more decimal places
    sens <- sum_rev[3,1] 
    spec <- sum_rev[4,1]
    max_sens <- max(sens,max_sens) ; min_sens <- min(sens,min_sens)  # Only assign min/max if smaller/greater than current assigned variable
    max_spec <- max(spec,max_spec) ; min_spec <- min(spec,min_spec)  # Only assign min/max if smaller/greater than current assigned variable
  
    # Append performance metrics to appropriate vectors
    sens_interim <- append(sens_interim, sens) ; spec_interim <- append(spec_interim, spec) ; auc_interim <- append(auc_interim, auc)
    mcc_interim <- append(mcc_interim, mcc_value) ; acc_interim <- append(acc_interim, acc) ; f1_interim <- append(f1_interim, f1)

    n <- n + 1    # Count for number of randomForest models generated

  }

  # Calculate averages across 100 RF simulations for OOB
  oob = mean(err[,1])  # Average of OOB error
  no = mean(err[,2])   # Average of No classification error
  yes = mean(err[,3])  # Average of Yes classification error
  err_final = rbind(err, c(oob,no,yes))   # Append averages to the end of individual model values
  
  # Calculate averages across 100 RF simulations for variable importance
  num <- dim(total)[1]  # Number of features
  n <- 1
  Importance = c()  # Empty vector for average variable importance
  
  while (n <= num) {
    i <- mean(total[n,])  # Average of variable importance for feature i 
    Importance <- c(Importance, as.numeric(i))
    n <- n + 1  
  }
  
  total_final = cbind(total, Importance)   # Append averages as separate column to individual model values
  
  # Calculate prediction success rate
  sens <- mean(sens_interim) ; spec <- mean(spec_interim)  # Take averages of all values calculated
  sens_stat <- paste(min_sens, sens, max_sens, sep=',') ; spec_stat <- paste(min_spec, spec, max_spec, sep=',')  # Combine final values into one variable
  write("Min Sensitivity,Avg Sensitivity,Max Sensitivity",file=file5,append=TRUE) ; write(sens_stat,file=file5,append=TRUE)  # Performance metric file
  write("Min Specificity,Avg Specificity,Max Specificity",file=file5,append=TRUE) ; write(spec_stat,file=file5,append=TRUE)  # Performance metric file
  
  f1_final <- mean(f1_interim) ; f1_stat <- paste(min_f1,f1_final,max_f1,sep=',')         # Take averages of all values calculated and 
  acc_final <- mean(acc_interim) ; acc_stat <- paste(min_acc,acc_final,max_acc,sep=',')   # combine final values into one variable.
  mcc_final <- mean(mcc_interim) ; mcc_stat <- paste(min_mcc,mcc_final,max_mcc,sep=',')   

  write("Min F1,Avg F1,Max F1",file=file5,append=TRUE) ; write(f1_stat,file=file5,append=TRUE)   # Performance metric file
  write("Min Accuracy,Avg Accuracy,Max Accuracy",file=file5,append=TRUE) ; write(acc_stat,file=file5,append=TRUE)
  write("Min MCC,Avg MCC,Max MCC",file=file5,append=TRUE) ; write(mcc_stat,file=file5,append=TRUE)
 
  auc <- mean(auc_interim) ; auc_stat <- paste(min_auc,auc,max_auc,sep=',')  # Combine final values into one variable
  write("Min AUC,Avg AUC,Max AUC",file=file5,append=TRUE) ; write(auc_stat,file=file5,append=TRUE)  # Performance metric file

  # Export remaining files
  write.csv(total_final,file2)   # Variable importance file  
  write.csv(err_final,file3)     # OOB error file
  write.csv(pred_final,file4)    # Predictions file

}
